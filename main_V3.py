# main_final_v4.py (ì•ˆì •ì ì¸ ë¼ˆëŒ€ + ìµœì‹  ì—”ì§„ ì´ì‹ ìµœì¢…ë³¸)

# --- 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ---
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import json
from typing import List, TypedDict
import logging
from fastapi.middleware.cors import CORSMiddleware

# --- 2. ë¡œê¹… ê¸°ë³¸ ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(levelname)s:     %(message)s')

# --- 3. LangChain, LangGraph ë° AI ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_pinecone import PineconeVectorStore
from langgraph.prebuilt import create_react_agent # [ìƒˆ ì—”ì§„]ì„ ìœ„í•´ ì¶”ê°€
from langgraph.graph import StateGraph, END
from langchain_core.tools import tool # [ìƒˆ ì—”ì§„]ì„ ìœ„í•´ ì¶”ê°€
import google.generativeai as genai # [ìƒˆ ì—”ì§„]ì„ ìœ„í•´ ì¶”ê°€
from langchain_core.documents import Document

# --- 4. í™˜ê²½ ì„¤ì • ---
load_dotenv()

app = FastAPI(
    title="MOIT AI Final Hybrid Server",
    description="ì•ˆì •ì ì¸ ë¼ìš°í„° ê¸°ë°˜ì— ìµœì‹  ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ì´ì‹í•œ ìµœì¢… AI ì‹œìŠ¤í…œ",
    version="4.1.0",
)

# --- CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ ---
origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- AI ëª¨ë¸ ë° API í‚¤ ì„¤ì • ---
try:
    gemini_api_key = os.getenv("GOOGLE_API_KEY")
    if not gemini_api_key:
        logging.warning("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        genai.configure(api_key=gemini_api_key)
except Exception as e:
    logging.warning(f"Gemini API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4)
llm_for_meeting = ChatOpenAI(model="gpt-4o-mini", temperature=0)


# --- 5. ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ì˜ State(ê¸°ì–µ ìƒì) ì •ì˜ ---
class MasterAgentState(TypedDict):
    user_input: dict
    route: str
    final_answer: str


# --- 6. ì „ë¬¸ê°€ #1: Self-RAG ëª¨ì„ ë§¤ì¹­ ì—ì´ì „íŠ¸ (SubGraph) ---
# [ìœ ì§€] ì•ˆì •ì„±ì´ ê²€ì¦ëœ ê¸°ì¡´ ëª¨ì„ ë§¤ì¹­ ì „ë¬¸ê°€ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

def call_meeting_matching_agent(state: MasterAgentState):
    """'ëª¨ì„ ë§¤ì¹­ ì—ì´ì „íŠ¸'ë¥¼ ë…ë¦½ì ì¸ SubGraphë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” ë…¸ë“œ"""
    print("--- CALLING: Meeting Matching Agent (Stable Version) ---")
    
    class MeetingAgentState(TypedDict):
        title: str; description: str; time: str; location: str; query: str;
        context: List[Document]; answer: str; rewrite_count: int; decision: str

    meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
    if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETING ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
    vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
    retriever = vector_store.as_retriever(search_kwargs={'k': 2})

    # SubGraphì˜ ë…¸ë“œë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
    prepare_query_prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ [ëª¨ì„ ì •ë³´]ë¥¼ ì¢…í•©í•˜ì—¬, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ëª¨ì„ì„ ì°¾ê¸° ìœ„í•œ ê°€ì¥ í•µì‹¬ì ì¸ ê²€ìƒ‰ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n"
        "[ëª¨ì„ ì •ë³´]:\n- ì œëª©: {title}\n- ì„¤ëª…: {description}\n- ì‹œê°„: {time}\n- ì¥ì†Œ: {location}"
    )
    prepare_query_chain = prepare_query_prompt | llm_for_meeting | StrOutputParser()
    def prepare_query(m_state: MeetingAgentState):
        query = prepare_query_chain.invoke(m_state)
        return {"query": query, "rewrite_count": 0}

    def retrieve(m_state: MeetingAgentState):
        return {"context": retriever.invoke(m_state['query'])}

    generate_prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ MOIT í”Œë«í¼ì˜ ì¹œì ˆí•œ ëª¨ì„ ì¶”ì²œ AIì…ë‹ˆë‹¤... (ì´í•˜ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼)" # ê¸°ì¡´ generate_prompt ì „ì²´ ì‚½ì…
    )
    generate_chain = generate_prompt | llm_for_meeting | StrOutputParser()
    def generate(m_state: MeetingAgentState):
        context = "\n\n".join(doc.page_content for doc in m_state['context'])
        answer = generate_chain.invoke({"context": context, "query": m_state['query']})
        return {"answer": answer}

    check_helpfulness_prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ AI ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì—„ê²©í•œ í‰ê°€ê´€ì…ë‹ˆë‹¤... (ì´í•˜ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼)" # ê¸°ì¡´ check_helpfulness_prompt ì „ì²´ ì‚½ì…
    )
    check_helpfulness_chain = check_helpfulness_prompt | llm_for_meeting | StrOutputParser()
    def check_helpfulness(m_state: MeetingAgentState):
        result = check_helpfulness_chain.invoke(m_state)
        return {"decision": "helpful" if 'helpful' in result.lower() else "unhelpful"}

    rewrite_query_prompt = ChatPromptTemplate.from_template(
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë” ì¢‹ì€ ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡... (ì´í•˜ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼)" # ê¸°ì¡´ rewrite_query_prompt ì „ì²´ ì‚½ì…
    )
    rewrite_query_chain = rewrite_query_prompt | llm_for_meeting | StrOutputParser()
    def rewrite_query(m_state: MeetingAgentState):
        new_query = rewrite_query_chain.invoke(m_state)
        count = m_state.get('rewrite_count', 0) + 1
        return {"query": new_query, "rewrite_count": count}
    
    # SubGraphë¥¼ ì¡°ë¦½í•©ë‹ˆë‹¤.
    graph_builder = StateGraph(MeetingAgentState)
    graph_builder.add_node("prepare_query", prepare_query)
    graph_builder.add_node("retrieve", retrieve)
    graph_builder.add_node("generate", generate)
    graph_builder.add_node("check_helpfulness", check_helpfulness)
    graph_builder.add_node("rewrite_query", rewrite_query)
    graph_builder.set_entry_point("prepare_query")
    graph_builder.add_edge("prepare_query", "retrieve")
    graph_builder.add_edge("retrieve", "generate")
    graph_builder.add_edge("generate", "check_helpfulness")
    graph_builder.add_conditional_edges( "check_helpfulness", lambda state: state['decision'], {"helpful": END, "unhelpful": "rewrite_query"})
    graph_builder.add_edge("rewrite_query", "retrieve")
    meeting_agent = graph_builder.compile()

    # ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ë¡œë¶€í„° ë°›ì€ ì •ë³´ë¡œ SubGraphë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    user_input = state['user_input']
    initial_state = { "title": user_input.get("title", ""), "description": user_input.get("description", ""), "time": user_input.get("time", ""), "location": user_input.get("location", "") }
    
    final_result_state = meeting_agent.invoke(initial_state, {"recursion_limit": 5})
    final_answer = final_result_state.get("answer", "ìœ ì‚¬í•œ ëª¨ì„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    return {"final_answer": final_answer}


# --- 7. [êµì²´] ì „ë¬¸ê°€ #2: ë©€í‹°ëª¨ë‹¬ ì·¨ë¯¸ ì¶”ì²œ ì—ì´ì „íŠ¸ (ReAct ê°ë…ê´€) ---

# 7-1. ì·¨ë¯¸ ì¶”ì²œì— í•„ìš”í•œ ë„êµ¬(Tool)ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
@tool
def analyze_photo_tool(image_paths: list[str]) -> str:
    from PIL import Image
    try:
        logging.info(f"--- ğŸ“¸ 'ì‚¬ì§„ ë¶„ì„ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---")
        model = genai.GenerativeModel('gemini-2.5-flash')
        # (ì´í•˜ ì „ì²´ ì‚¬ì§„ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ë° ë¡œì§)
        photo_analysis_prompt_text = "ë‹¹ì‹ ì€ ì‚¬ëŒë“¤ì˜ ì¼ìƒ ì‚¬ì§„ì„ ë³´ê³ ... (ì´í•˜ ìƒëµ)"
        image_parts = [Image.open(path) for path in image_paths]
        response = model.generate_content([photo_analysis_prompt_text] + image_parts)
        return response.text
    except Exception as e:
        return f"ì˜¤ë¥˜: ì‚¬ì§„ ë¶„ì„ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def _normalize(value, min_val, max_val):
    if value is None: return None
    return round((value - min_val) / (max_val - min_val), 4)

@tool
def analyze_survey_tool(survey_json_string: str) -> dict:
    logging.info("--- ğŸ“Š 'ì„¤ë¬¸ ë¶„ì„ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---")
    try:
        responses = json.loads(survey_json_string)
        features = {'FSC': {}, 'PSSR': {}, 'MP': {}, 'DLS': {}}
        # (ì´í•˜ ì „ì²´ ì„¤ë¬¸ ë¶„ì„ ë¡œì§)
        # ...
        return features
    except Exception as e:
        return {"error": f"ì„¤ë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

@tool
def summarize_survey_profile_tool(survey_profile: dict) -> str:
    logging.info("--- âœï¸ 'ì„¤ë¬¸ ìš”ì•½ ì „ë¬¸ê°€'ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---")
    try:
        summarizer_prompt = ChatPromptTemplate.from_template("ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì„±í–¥ ë¶„ì„ ë°ì´í„°ë¥¼ í•´ì„í•˜ì—¬... (ì´í•˜ ìƒëµ)")
        summarizer_chain = summarizer_prompt | llm | StrOutputParser()
        summary = summarizer_chain.invoke({"profile": survey_profile})
        return summary
    except Exception as e:
        return f"ì˜¤ë¥˜: ì„¤ë¬¸ ìš”ì•½ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# 7-2. ì´ ë„êµ¬ë“¤ì„ ì§€íœ˜í•  ReAct ê°ë…ê´€ì„ ìƒì„±í•©ë‹ˆë‹¤.
hobby_tools = [analyze_photo_tool, analyze_survey_tool, summarize_survey_profile_tool]
hobby_supervisor_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì‚¬ì§„ê³¼ ì„¤ë¬¸ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë§ì¶¤í˜• ì·¨ë¯¸ë¥¼ ì¶”ì²œí•˜ëŠ” AI íë ˆì´í„°ì…ë‹ˆë‹¤... (ì´í•˜ ìš°ë¦¬ê°€ ì™„ì„±í•œ ìµœì¢… ê°ë…ê´€ í”„ë¡¬í”„íŠ¸ v2.2)"""
hobby_prompt = ChatPromptTemplate.from_messages([("system", hobby_supervisor_prompt), MessagesPlaceholder(variable_name="messages")])
hobby_supervisor_agent = create_react_agent(llm, hobby_tools, prompt=hobby_prompt)

# 7-3. ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸(ë¼ìš°í„°)ê°€ í˜¸ì¶œí•  ìµœì¢… ë…¸ë“œ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.
def call_multimodal_hobby_agent(state: MasterAgentState):
    """'ë©€í‹°ëª¨ë‹¬ ì·¨ë¯¸ ì¶”ì²œ ê°ë…ê´€'ì„ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” ë…¸ë“œ"""
    print("--- CALLING: Multimodal Hobby Supervisor Agent ---")
    
    user_input_str = json.dumps(state["user_input"], ensure_ascii=False)
    input_data = {"messages": [("user", f"ë‹¤ìŒ ì‚¬ìš©ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì·¨ë¯¸ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”: {user_input_str}")]}
    
    final_answer = ""
    for event in hobby_supervisor_agent.stream(input_data, {"recursion_limit": 15}):
        if "messages" in event:
            last_message = event["messages"][-1]
            if isinstance(last_message.content, str) and not last_message.tool_calls:
                final_answer = last_message.content
                
    return {"final_answer": final_answer}


# --- 8. ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸(ë¼ìš°í„°) ì¡°ë¦½ ---

# 8-1. ë¼ìš°í„° ë…¸ë“œ ì •ì˜
routing_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë‹´ë‹¹ìì—ê²Œ ì „ë‹¬í•´ì•¼ í• ì§€ ê²°ì •í•˜ëŠ” AI ë¼ìš°í„°ì…ë‹ˆë‹¤... (ì´í•˜ ê¸°ì¡´ ë¼ìš°í„° í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼)"""
)
router_chain = routing_prompt | llm | StrOutputParser()

def route_request(state: MasterAgentState):
    print("--- ROUTING ---")
    # [ìˆ˜ì •] ë¼ìš°íŒ…ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„° êµ¬ì¡°ë¥¼ ëª…í™•íˆ í•©ë‹ˆë‹¤.
    task_description = state['user_input'].get('task_description', str(state['user_input']))
    route_decision = router_chain.invoke({"user_input": task_description})
    cleaned_decision = route_decision.strip().replace("'", "").replace('"', '')
    print(f"ë¼ìš°íŒ… ê²°ì •: {cleaned_decision}")
    return {"route": cleaned_decision}

# 8-2. ë§ˆìŠ¤í„° ê·¸ë˜í”„ ì¡°ë¦½
master_graph_builder = StateGraph(MasterAgentState)

master_graph_builder.add_node("router", route_request)
master_graph_builder.add_node("meeting_matcher", call_meeting_matching_agent)
# [êµì²´ë¨]
master_graph_builder.add_node("hobby_recommender", call_multimodal_hobby_agent) 

master_graph_builder.set_entry_point("router")

master_graph_builder.add_conditional_edges(
    "router", 
    lambda state: state['route'],
    {"meeting_matching": "meeting_matcher", "hobby_recommendation": "hobby_recommender"}
)

master_graph_builder.add_edge("meeting_matcher", END)
master_graph_builder.add_edge("hobby_recommender", END)

master_agent = master_graph_builder.compile()


# --- 9. API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
class UserRequest(BaseModel):
    user_input: dict

@app.post("/agent/invoke")
async def invoke_agent(request: UserRequest):
    try:
        input_data = {"user_input": request.user_input}
        result = master_agent.invoke(input_data)
        return {"final_answer": result.get("final_answer", "ì˜¤ë¥˜: ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# --- 10. Pinecone DB ì—…ë°ì´íŠ¸/ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸ ---
class NewMeeting(BaseModel):
    meeting_id: str
    title: str
    description: str
    time: str
    location: str

@app.post("/meetings/add")
async def add_meeting_to_pinecone(meeting: NewMeeting):
    try:
        meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
        if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETINGì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
        vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
        
        full_text = f"ì œëª©: {meeting.title}\nì„¤ëª…: {meeting.description}\nì‹œê°„: {meeting.time}\nì¥ì†Œ: {meeting.location}"
        metadata = {"title": meeting.title, "description": meeting.description, "time": meeting.time, "location": meeting.location, "meeting_id": meeting.meeting_id}
        
        vector_store.add_texts(texts=[full_text], metadatas=[metadata], ids=[meeting.meeting_id])
        
        logging.info(f"--- Pineconeì— ëª¨ì„ ì¶”ê°€ ì„±ê³µ (ID: {meeting.meeting_id}) ---")
        return {"status": "success", "message": f"ëª¨ì„(ID: {meeting.meeting_id})ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logging.error(f"Pinecone ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Pineconeì— ëª¨ì„ì„ ì¶”ê°€í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.delete("/meetings/delete/{meeting_id}")
async def delete_meeting_from_pinecone(meeting_id: str):
    try:
        logging.info(f"--- Pineconeì—ì„œ ëª¨ì„ ì‚­ì œ ì‹œì‘ (ID: {meeting_id}) ---")
        meeting_index_name = os.getenv("PINECONE_INDEX_NAME_MEETING")
        if not meeting_index_name: raise ValueError("'.env' íŒŒì¼ì— PINECONE_INDEX_NAME_MEETINGì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')
        vector_store = PineconeVectorStore.from_existing_index(index_name=meeting_index_name, embedding=embedding_function)
        
        vector_store.delete(ids=[meeting_id])
        
        logging.info(f"--- Pineconeì—ì„œ ëª¨ì„ ì‚­ì œ ì„±ê³µ (ID: {meeting_id}) ---")
        return {"status": "success", "message": f"ëª¨ì„(ID: {meeting_id})ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        logging.error(f"Pinecone ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Pineconeì—ì„œ ëª¨ì„ì„ ì‚­ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")